
# Name the components on this agent
a1.sources = r1
a1.sinks = k1 k2 k3
a1.channels = c1 c2 c3

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.channels = channel-1
a1.sources.r1.spoolDir = /home/bigdata/flume/source

# Describe regex_extractor to extract different patterns
# Capture event attribute and add it as event header with attribure name as "role"
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = regex_extractor
a1.sources.r1.interceptors.i1.regex = ^(\\d)
a1.sources.r1.interceptors.i1.serializers = t
a1.sources.r1.interceptors.i1.serializers.t.name = role

a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /user/bigdata/pec2/ex3/tipo1
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.fileSuffix = .txt

a1.sinks.k2.channel = c2
a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = /user/bigdata/pec2/ex3/tipo2
a1.sinks.k2.hdfs.fileType = DataStream
a1.sinks.k2.hdfs.writeFormat = Text
a1.sinks.k2.hdfs.fileSuffix = .txt

a1.sinks.k3.channel = c3
a1.sinks.k3.type = hdfs
a1.sinks.k3.hdfs.path = /user/bigdata/pec2/ex3/tipo3
a1.sinks.k3.hdfs.fileType = DataStream
a1.sinks.k3.hdfs.writeFormat = Text
a1.sinks.k3.hdfs.fileSuffix = .txt

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 100
a1.channels.c2.type = memory
a1.channels.c2.capacity = 100
a1.channels.c3.type = memory
a1.channels.c3.capacity = 100

# Define channel selector and define mapping
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = role
a1.sources.r1.selector.mapping.1 = c1
a1.sources.r1.selector.mapping.2 = c2
a1.sources.r1.selector.mapping.3 = c3

# Bind the source and sink to the channel
a1.sources.r1.channels = c1 c2 c3


